{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05_Evaluate.ipynb : √âvaluation des recommandations\n",
    "\n",
    "Ce notebook a pour objectif de mesurer la qualit√© des listes de recommandations g√©n√©r√©es (CF, Content-Based, Hybrid) en les comparant aux interactions r√©elles du jeu de test.\n",
    "\n",
    "**√âtapes principales :**\n",
    "1. D√©finition des m√©triques (Precision@K, NDCG@K).\n",
    "2. Impl√©mentation de fonctions utilitaires pour calculer ces m√©triques.\n",
    "3. Fonction `evaluate` pour agr√©ger les scores sur tous les utilisateurs.\n",
    "4. Chargement des fichiers de recommandations et v√©rit√© terrain.\n",
    "5. Calcul et affichage des r√©sultats par mod√®le."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fonctions d'√©valuation\n",
    "### 2.1. Precision@K\n",
    "\n",
    "Calcule le ratio d'items dans les k premi√®res positions qui apparaissent dans la v√©rit√© terrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(ranked_list, ground_truth, k):\n",
    "    \"\"\"\n",
    "    :param ranked_list: liste des item_id recommand√©s class√©s\n",
    "    :param ground_truth: ensemble des item_id r√©ellement vus\n",
    "    :param k: nombre de positions √† consid√©rer\n",
    "    :return: pr√©cision\n",
    "    \"\"\"\n",
    "    hit = sum(1 for item in ranked_list[:k] if item in ground_truth)\n",
    "    return hit / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. DCG@K et NDCG@K\n",
    "\n",
    "- **DCG@K** : somme des gains pond√©r√©s par le log de la position.\n",
    "- **IDCG@K** : DCG maximum possible (sc√©nario id√©al).\n",
    "- **NDCG@K** : DCG normalis√© par IDCG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(ranked_list, ground_truth, k):\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(ranked_list[:k]):\n",
    "        if item in ground_truth:\n",
    "            dcg += 1.0 / np.log2(i + 2)\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def idcg_at_k(ground_truth, k):\n",
    "    ideal_hits = min(len(ground_truth), k)\n",
    "    return sum(1.0 / np.log2(i + 2) for i in range(ideal_hits))\n",
    "\n",
    "\n",
    "def ndcg_at_k(ranked_list, ground_truth, k):\n",
    "    idcg = idcg_at_k(ground_truth, k)\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg_at_k(ranked_list, ground_truth, k) / idcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fonction d'√©valuation globale\n",
    "\n",
    "La fonction `evaluate` parcourt chaque utilisateur, r√©cup√®re ses recommandations et la v√©rit√© terrain, puis calcule la pr√©cision et le NDCG, et renvoie la moyenne globale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(recs_df, truth_df, k=10):\n",
    "    \"\"\"\n",
    "    :param recs_df: DataFrame avec colonnes ['user_id','item_id','rank'] des recommandations\n",
    "    :param truth_df: DataFrame avec colonnes ['user_id','item_id'] des interactions r√©elles\n",
    "    :param k: cutoff pour les m√©triques\n",
    "    :return: dict avec les scores moyens {'precision': ..., 'ndcg': ...}\n",
    "    \"\"\"\n",
    "    users = recs_df['user_id'].unique()\n",
    "    precisions, ndcgs = [], []\n",
    "    for u in users:\n",
    "        preds = recs_df[recs_df.user_id == u].sort_values('rank')['item_id'].tolist()\n",
    "        actual = set(truth_df[truth_df.user_id == u]['item_id'])\n",
    "        precisions.append(precision_at_k(preds, actual, k))\n",
    "        ndcgs.append(ndcg_at_k(preds, actual, k))\n",
    "    return {\n",
    "        'precision': np.mean(precisions),\n",
    "        'ndcg': np.mean(ndcgs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chargement des donn√©es et √©valuation des mod√®les\n",
    "\n",
    "On charge la v√©rit√© terrain (`interactions_test.csv` ou `small_matrix.csv`) et les fichiers de recommandations (`submission_cf.csv`, etc.), puis on affiche les scores pour chaque approche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Mod√®le CF\n",
      "  - Precision@10 : 0.4505\n",
      "  - NDCG@10      : 0.4884\n",
      "\n",
      "üìä Mod√®le CONTENT\n",
      "  - Precision@10 : 0.4851\n",
      "  - NDCG@10      : 0.5547\n",
      "\n",
      "üìä Mod√®le HYBRID\n",
      "  - Precision@10 : 0.4825\n",
      "  - NDCG@10      : 0.5262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chargement de la v√©rit√© terrain\n",
    "truth = pd.read_csv(\"../data/small_matrix.csv\").rename(columns={\"video_id\": \"item_id\"})\n",
    "\n",
    "# Liste des mod√®les √† √©valuer\n",
    "models = [\"cf\", \"content\", \"hybrid\"]\n",
    "for name in models:\n",
    "    # Chargement des recommandations\n",
    "    recs = pd.read_csv(f\"submission_{name}.csv\").rename(columns={\"video_id\": \"item_id\"})\n",
    "    # Calcul des m√©triques\n",
    "    scores = evaluate(recs, truth, k=10)\n",
    "    # Affichage des r√©sultats\n",
    "    print(f\"üìä Mod√®le {name.upper()}\")\n",
    "    print(f\"  - Precision@10 : {scores['precision']:.4f}\")\n",
    "    print(f\"  - NDCG@10      : {scores['ndcg']:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
