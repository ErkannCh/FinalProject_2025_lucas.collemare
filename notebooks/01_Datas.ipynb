{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Pré-traitement des données\n",
    "\n",
    "Ce notebook a pour objectif de charger, nettoyer et prétraiter les fichiers bruts du dataset KuaiRec afin de fournir des jeux de données prêts à l'emploi pour les étapes ultérieures du pipeline (construction des features, modélisation, etc.).\n",
    "\n",
    "**Fichiers sources :**\n",
    "- `big_matrix.csv` : interactions historiques (toutes interactions utilisateurs-vidéos)\n",
    "- `small_matrix.csv` : sous-échantillon pour test et évaluation\n",
    "- `social_network.csv` : graphe d’amitié des utilisateurs\n",
    "- `item_categories.csv` : catégorisation des vidéos et traits (liste de features)\n",
    "- `user_features.csv` : attributs des utilisateurs\n",
    "- `item_daily_features.csv` : statistiques quotidiennes par vidéo\n",
    "\n",
    "**Formats de sortie :** fichiers Parquet pour accélérer la réutilisation en mémoire créé dans un dossier 'preprocessed/'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fonction de chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"../data\"):\n",
    "    print(\"Loading datas...\")\n",
    "    # Chargement des tables d'interactions\n",
    "    big_matrix = pd.read_csv(f\"{data_dir}/big_matrix.csv\")\n",
    "    small_matrix = pd.read_csv(f\"{data_dir}/small_matrix.csv\")\n",
    "    # Chargement du réseau social, conversion de la colonne friend_list en liste Python\n",
    "    social_network = pd.read_csv(f\"{data_dir}/social_network.csv\")\n",
    "    social_network[\"friend_list\"] = social_network[\"friend_list\"].map(eval)\n",
    "    # Chargement des catégories et parsing des features\n",
    "    item_categories = pd.read_csv(f\"{data_dir}/item_categories.csv\")\n",
    "    item_categories[\"feat\"] = item_categories[\"feat\"].map(eval)\n",
    "    # Chargement des features utilisateurs et journalières\n",
    "    user_features = pd.read_csv(f\"{data_dir}/user_features.csv\")\n",
    "    item_daily_feat = pd.read_csv(f\"{data_dir}/item_daily_features.csv\")\n",
    "    print(\"All data loaded.\")\n",
    "    # Retour sous forme de dictionnaire\n",
    "    return {\n",
    "        \"big_matrix\": big_matrix,\n",
    "        \"small_matrix\": small_matrix,\n",
    "        \"social_network\": social_network,\n",
    "        \"item_categories\": item_categories,\n",
    "        \"user_features\": user_features,\n",
    "        \"item_daily_feat\": item_daily_feat\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exécution du chargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datas...\n",
      "All data loaded.\n"
     ]
    }
   ],
   "source": [
    "dfs = load_data()\n",
    "big = dfs[\"big_matrix\"]\n",
    "small = dfs[\"small_matrix\"]\n",
    "items = dfs[\"item_categories\"]\n",
    "users = dfs[\"user_features\"]\n",
    "daily = dfs[\"item_daily_feat\"]\n",
    "social = dfs[\"social_network\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Nettoyage des données d'interactions\n",
    "- Suppression des lignes ayant des clés manquantes (`user_id`, `video_id`, `timestamp`)\n",
    "- Création d'une copie de `small` pour éviter les avertissements pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "big = big.dropna(subset=[\"user_id\", \"video_id\", \"timestamp\"])\n",
    "small = small.dropna(subset=[\"user_id\", \"video_id\", \"timestamp\"]).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conversion des timestamps\n",
    "Les timestamps sont au format integer (secondes depuis Epoch). On les convertit en datetime pour faciliter les calculs temporels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_148935/3233873522.py:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2020-07-04 21:27:48.378000021', '2020-07-04 21:28:00.056999922',\n",
      " '2020-07-04 21:29:09.479000092', '2020-07-04 21:30:43.285000086',\n",
      " '2020-07-04 21:35:43.459000111', '2020-07-04 21:36:00.773000002',\n",
      " '2020-07-04 21:36:47.740999937', '2020-07-04 21:49:27.964999914',\n",
      " '2020-07-04 21:49:41.762000084', '2020-07-04 21:57:26.581000090',\n",
      " ...\n",
      " '2020-09-01 09:23:27.664000034', '2020-09-01 10:31:41.650000095',\n",
      " '2020-09-01 10:35:43.855000019', '2020-09-01 11:35:45.312999964',\n",
      " '2020-09-01 11:59:25.707999945', '2020-09-01 12:06:35.983999968',\n",
      " '2020-09-02 06:44:51.342000008', '2020-09-03 00:45:01.473999977',\n",
      " '2020-09-04 14:56:32.020999908', '2020-09-04 16:32:09.154000044']\n",
      "Length: 4494578, dtype: datetime64[ns]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  small.loc[:, \"timestamp\"] = pd.to_datetime(small[\"timestamp\"], unit=\"s\")\n"
     ]
    }
   ],
   "source": [
    "big[\"timestamp\"] = pd.to_datetime(big[\"timestamp\"], unit=\"s\")\n",
    "small.loc[:, \"timestamp\"] = pd.to_datetime(small[\"timestamp\"], unit=\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Enrichissement par jointure de catégories\n",
    "\n",
    "Pour chaque interaction, on attache la liste de features de la vidéo (`feat`) via une jointure à gauche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "big = big.merge(items[[\"video_id\", \"feat\"]], on=\"video_id\", how=\"left\")\n",
    "small = small.merge(items[[\"video_id\", \"feat\"]], on=\"video_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export vers Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed files exported\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"preprocessed\", exist_ok=True)\n",
    "\n",
    "big.to_parquet(\"preprocessed/big_matrix.parquet\", index=False)\n",
    "small.to_parquet(\"preprocessed/small_matrix.parquet\", index=False)\n",
    "users.to_parquet(\"preprocessed/user_features.parquet\", index=False)\n",
    "items.to_parquet(\"preprocessed/item_categories.parquet\", index=False)\n",
    "daily.to_parquet(\"preprocessed/item_daily_features.parquet\", index=False)\n",
    "social.to_parquet(\"preprocessed/social_network.parquet\", index=False)\n",
    "\n",
    "print(f\"Preprocessed files exported\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
