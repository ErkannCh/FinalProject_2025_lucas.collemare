{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Construction des features\n",
    "\n",
    "Ce notebook a pour objectif de transformer les données prétraitées en features exploitables par les modèles de recommandation. Nous construisons :\n",
    "\n",
    "- **Matrice d'interaction** user–item sparse pondérée et normalisée.\n",
    "- **Cartographies** des identifiants utilisateurs et vidéos.\n",
    "- **Features additionnels** : nombre total d'interactions par utilisateur, moyenne quotidienne de vues par vidéo.\n",
    "\n",
    "Les fichiers générés seront sauvegardés dans le dossier `features/` pour être réutilisés dans Model_Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import normalize\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fonction `build_interaction_matrix`\n",
    "\n",
    "Cette fonction construit la matrice user–item pondérée selon :\n",
    "\n",
    "1. **Filtrage** : on conserve uniquement les utilisateurs et vidéos ayant au moins `min_interactions`.\n",
    "2. **Mapping** : on crée deux dictionnaires `user_map` et `video_map` pour indexer les IDs.\n",
    "3. **Pondération** :\n",
    "   - `play_duration_s` (durée de vue en secondes)\n",
    "   - **Source weight** (`alpha`) pour ajuster l’importance globale des interactions.\n",
    "   - **Décroissance temporelle** : poids exponentiel selon l’âge de l’interaction (`decay`).\n",
    "4. **Construction** : on assemble un `csr_matrix` et on normalise chaque ligne (L1) pour obtenir des profils comparables.\n",
    "\n",
    "**Arguments** :\n",
    "- `train_df` : DataFrame d’interactions contenant `user_id`, `video_id`, `play_duration`, `timestamp`.\n",
    "- `min_interactions` : seuil minimal pour filtrer.\n",
    "- `alpha` : coefficient de pondération source.\n",
    "- `decay` : taux de décroissance exponentielle temporelle.\n",
    "\n",
    "**Retour** :\n",
    "- `mat` : matrice user–item normalisée (csr_matrix)\n",
    "- `user_map`, `video_map` : dicts pour passer des IDs crues aux indices de la matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_interaction_matrix(train_df, min_interactions=5, alpha=1.0, decay=1e-7):\n",
    "    # Copie pour ne pas modifier l'original\n",
    "    df = train_df.copy()\n",
    "    # Poids constant pour chaque interaction (peut intégrer d'autres sources)\n",
    "    df[\"source_weight\"] = alpha\n",
    "\n",
    "    # Filtrage des utilisateurs et vidéos trop rares\n",
    "    active_users  = df[\"user_id\"].value_counts()[lambda x: x >= min_interactions].index\n",
    "    active_videos = df[\"video_id\"].value_counts()[lambda x: x >= min_interactions].index\n",
    "    df = df[df[\"user_id\"].isin(active_users) & df[\"video_id\"].isin(active_videos)]\n",
    "\n",
    "    # Création des mappings id -> index\n",
    "    user_map  = {u: i for i, u in enumerate(df[\"user_id\"].unique())}\n",
    "    video_map = {v: k for k, v in enumerate(df[\"video_id\"].unique())}\n",
    "    df[\"uidx\"] = df[\"user_id\"].map(user_map)\n",
    "    df[\"vidx\"] = df[\"video_id\"].map(video_map)\n",
    "\n",
    "    # Conversion de la durée de lecture en secondes\n",
    "    df[\"play_duration_s\"] = df[\"play_duration\"] / 1000.0\n",
    "\n",
    "    # Calcul de l'âge de l'interaction pour la décroissance temporelle\n",
    "    max_ts = df[\"timestamp\"].astype(int).max() / 1e9\n",
    "    df[\"age\"] = (max_ts - df[\"timestamp\"].astype(int) / 1e9)\n",
    "    df[\"time_weight\"] = np.exp(-decay * df[\"age\"])\n",
    "\n",
    "    # Force des interactions : durée * source_weight * time_weight\n",
    "    df[\"strength\"] = df[\"play_duration_s\"] * df[\"source_weight\"] * df[\"time_weight\"]\n",
    "\n",
    "    # Assemblage de la matrice creuse\n",
    "    rows = df[\"uidx\"].to_numpy()\n",
    "    cols = df[\"vidx\"].to_numpy()\n",
    "    data = df[\"strength\"].to_numpy()\n",
    "    mat = sp.csr_matrix((data, (rows, cols)), shape=(len(user_map), len(video_map)))\n",
    "\n",
    "    # Normalisation L1 par ligne\n",
    "    mat = normalize(mat, norm=\"l1\", axis=1)\n",
    "    return mat, user_map, video_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement des données et construction de la matrice\n",
    "On charge la matrice complète prétraitée (`big_matrix`) et décide de construire la matrice d'interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "big = pd.read_parquet(\"preprocessed/big_matrix.parquet\")\n",
    "daily = pd.read_parquet(\"preprocessed/item_daily_features.parquet\")\n",
    "\n",
    "# Construction de la matrice sparse et des mappings\n",
    "mat, user_map, video_map = build_interaction_matrix(big, alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sauvegarde des features\n",
    "\n",
    "Pour réutiliser rapidement ces objets, on les enregistre dans `features/` :\n",
    "- Matrice d'interaction au format `.npz` (scipy)\n",
    "- Mappings utilisateur/vidéo au format `joblib`\n",
    "- Features additionnels calculés par agrégation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sauvegardés dans features/\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"features\", exist_ok=True)\n",
    "\n",
    "# Sauvegarde de la matrice d'interaction et des mappings\n",
    "sp.save_npz(\"features/interaction_matrix.npz\", mat)\n",
    "joblib.dump(user_map, \"features/user_map.pkl\")\n",
    "joblib.dump(video_map, \"features/video_map.pkl\")\n",
    "\n",
    "# Feature : nombre total d'interactions par utilisateur\n",
    "df_users = big.groupby(\"user_id\").size().rename(\"total_interactions\").reset_index()\n",
    "df_users.to_parquet(\"features/user_features.parquet\", index=False)\n",
    "\n",
    "# Feature : moyenne quotidienne de vues par vidéo\n",
    "# Agrégation de 'play_cnt' dans daily\n",
    "df_videos = daily.groupby(\"video_id\")[\"play_cnt\"].mean().rename(\"avg_daily_plays\").reset_index()\n",
    "df_videos.to_parquet(\"features/video_features.parquet\", index=False)\n",
    "\n",
    "print(\"Features sauvegardés dans features/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
